{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86de6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, MT5Tokenizer, MT5Config\n",
    ")\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import gc\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "device, use_gpu = (\"cuda:0\", True) if torch.cuda.is_available() else (\"cpu\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2d7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec62d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "load tokenizer done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "checkpoint = \"VietAI/vit5-base\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "print('load model done')\n",
    "tokenizer = MT5Tokenizer.from_pretrained(checkpoint)\n",
    "print('load tokenizer done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9617d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 51\n",
      "total size of data is 1465800\n"
     ]
    }
   ],
   "source": [
    "PATHS = [\"data/question_full.txt\",\"data/sentence_full.txt\"]\n",
    "data = []\n",
    "cnt  = 0\n",
    "for path in PATHS:\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split('\\t')\n",
    "            if len(line) !=6 :\n",
    "                cnt += 1\n",
    "                continue\n",
    "            data.append(\n",
    "            {\n",
    "                'src': line[0],\n",
    "                'tgt': line[1]\n",
    "            })\n",
    "            data.append(\n",
    "            {\n",
    "                'src': line[1],\n",
    "                'tgt': line[2]\n",
    "            })\n",
    "            data.append(\n",
    "            {\n",
    "                'src': line[2],\n",
    "                'tgt': line[3]\n",
    "            })\n",
    "            data.append(\n",
    "            {\n",
    "                'src': line[4],\n",
    "                'tgt': line[5]\n",
    "            })\n",
    "print(f'error {cnt}')\n",
    "\n",
    "print(f'total size of data is {len(data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5edb76fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Thuy·∫øt ti·∫øn h√≥a c√≥ gi·∫£i th√≠ch ƒë∆∞·ª£c t·∫°i sao c√≥ nhi·ªÅu lo·∫°i th·ª±c v·∫≠t kh√¥ng?',\n",
       " 'sql': 'S·ª± ƒëa d·∫°ng c·ªßa c√°c lo√†i th·ª±c v·∫≠t ƒë∆∞·ª£c gi·∫£i th√≠ch b·∫±ng thuy·∫øt ti·∫øn h√≥a?'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1050000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae074a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = pd.DataFrame(data)\n",
    "tdata = tdata.reset_index()\n",
    "dataset = datasets.Dataset.from_pandas(tdata)\n",
    "\n",
    "train = dataset.train_test_split(\n",
    "    train_size=1463000, test_size=2800, seed=42\n",
    ")\n",
    "\n",
    "train_data = train['train']\n",
    "test_data = train['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8a6ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1463000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1463000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/workspace/student2/Trieu/NMT-with-Translation-Memory/chieunq_1/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_dataset(example):\n",
    "     return {'input': example['src'], 'target': example['tgt']}\n",
    "train_data = train_data.map(format_dataset, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(format_dataset, remove_columns=test_data.column_names)\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], pad_to_max_length=True, max_length=128)\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], pad_to_max_length=True, max_length=128)\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'], \n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "train_data = train_data.map(convert_to_features, batched=True, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(convert_to_features, batched=True, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be60357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1457332/3190919124.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2e3236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d1c8f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer,model=model)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"viT5-base-1\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=22859,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=45718,\n",
    "    eval_steps=22859,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "     group_by_length=True,\n",
    "    #fp16=True, \n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09133add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/student2/Trieu/NMT-with-Translation-Memory/chieunq_1/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48940' max='182876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48940/182876 4:48:47 < 13:10:21, 2.82 it/s, Epoch 0.54/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22859</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.234621</td>\n",
       "      <td>0.508800</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45718</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.217037</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.428000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da9b62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fae83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
